# Assistant FAQ Intelligent

Cet assistant IA a √©t√© con√ßu pour la **Communaut√© de Communes Val de Loire Num√©rique** afin d'automatiser les r√©ponses aux questions des citoyens concernant les d√©marches administratives (√©tat civil, urbanisme, transports, etc.).

Liens : 
* [front](https://projet-faq.vercel.app/)
* [api](https://api-projet-faq.vercel.app/docs)

---

## üöÄ Fonctionnement de l'Application

L'application repose sur une architecture **RAG (Retrieval-Augmented Generation)**, s√©lectionn√©e apr√®s un benchmark rigoureux de trois strat√©gies.

### Flux de traitement :
1. **Requ√™te Utilisateur** : Le citoyen pose une question via l'interface Next.js.
2. **Recherche S√©mantique (Retrieval)** : L'API convertit la question en vecteur (embedding) via le mod√®le `multilingual-e5-small` et interroge une base de donn√©es Postgres (Supabase) √©quip√©e de l'extension `pgvector` pour trouver les documents FAQ les plus pertinents.
3. **G√©n√©ration Contextuelle** : Les documents trouv√©s sont inject√©s dans un prompt syst√®me. Le LLM `Mistral-7B-Instruct-v0.2` g√©n√®re ensuite une r√©ponse pr√©cise, bas√©e uniquement sur ce contexte pour √©viter les hallucinations.
4. **Streaming** : La r√©ponse est renvoy√©e en temps r√©el √† l'utilisateur via un flux de donn√©es (streaming) pour une meilleure exp√©rience utilisateur.

![Architecture de l'Assistant FAQ Intelligent : Le Flux RAG](docs/infographieREADME.png)


---

## üèóÔ∏è Architecture Technique

Le projet utilise une approche **Serverless "Scale-to-Zero"**, garantissant un co√ªt nul au repos et une mont√©e en charge automatique.

- **Frontend** : Next.js h√©berg√© sur Vercel.
- **Backend** : API FastAPI h√©berg√©e sur Vercel.
- **Infrastructure IA** :
  - Modal : Pour le service de recherche et de vectorisation serverless.
  - Hugging Face Inference API : Pour l'ex√©cution du mod√®le Mistral 7B.
- **Base de donn√©es** : Supabase (Postgres).

---

## üõ†Ô∏è Stack Technologique

- **Langage** : Python 3.12+.
- **Gestionnaire de paquets** : `uv`.
- **Mod√®les** :
  - LLM : `Mistral-7B-Instruct-v0.2`.
  - Embeddings : `intfloat/multilingual-e5-small`.
- **Tests** : `Pytest` pour les tests unitaires et d'int√©gration.
- **CI/CD** : GitHub Actions pour le d√©ploiement automatique sur Vercel et Modal.

---

## üì¶ Installation et Utilisation

### Pr√©requis
- Python 3.12 et l'outil `uv`.
- Des comptes Hugging Face, Vercel, Modal et Supabase.

### Quick Start

```bash
# 1. Clonage du repo
git clone https://github.com/VestiC1/Projet-FAQ.git

# 2. Configuration : Cr√©er un fichier .env (cf .env.template)
cp .env.template .env
# Remplir les variables d'environnement

# 3. Initialisation
uv sync --all-extras

# 4. Lancement API
make app

# 5. Lancement Front
cd src/frontend
npm run dev
```